{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 1.13.1+cu117\n",
            "Cuda available: True\n",
            "Torch geometric version: 2.2.0\n",
            "Torch version: 1.13.1+cu117\n",
            "Cuda available: True\n",
            "Torch geometric version: 2.2.0\n"
          ]
        }
      ],
      "source": [
        "# SELF REMINDER. Copy the 'ocpa' directory to the forked one from github, so that I can push updates to github.\n",
        "# Python native\n",
        "import pickle\n",
        "from statistics import median, mean\n",
        "import pandas as pd\n",
        "# Data handling\n",
        "# Object centric process mining\n",
        "# import ocpa.algo.evaluation.precision_and_fitness.utils as evaluation_utils # COMMENTED OUT BY TIM\n",
        "# import ocpa.algo.evaluation.precision_and_fitness.evaluator as precision_fitness_evaluator # COMMENTED OUT BY TIM\n",
        "import ocpa.objects.log.importer.csv.factory as csv_import_factory\n",
        "import ocpa.algo.predictive_monitoring.factory as feature_factory\n",
        "from ocpa.objects.log.ocel import OCEL\n",
        "\n",
        "# # Simple machine learning models, procedure tools, and evaluation metrics\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# # Custom GNN tools\n",
        "# from gnn_utils import (\n",
        "#     generate_graph_dataset,\n",
        "#     # get_ordered_event_list,\n",
        "#     # visualize_graph,\n",
        "#     # show_remaining_times,\n",
        "#     # visualize_instance,\n",
        "#     # GraphDataLoader,\n",
        "#     # GCN,\n",
        "#     # evaluate_gnn,\n",
        "# )\n",
        "\n",
        "# PyG\n",
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "from replicating.ocpa_PyG_integration.EventGraphDataset import EventGraphDataset\n",
        "from replicating.ocpa_PyG_integration.EventSubGraphDataset import EventSubGraphDataset\n",
        "from replicating.model import GCN, GAT\n",
        "\n",
        "# Global variables\n",
        "from replicating.experiment_config import STORAGE_PATH, FEATURE_STORAGE_FILE, RANDOM_SEED, TARGET_LABEL\n",
        "\n",
        "\n",
        "filename = \"data/adams/example_logs/mdl/BPI2017-Final.csv\"\n",
        "object_types = [\"application\", \"offer\"]\n",
        "parameters = {\n",
        "    \"obj_names\": object_types,\n",
        "    \"val_names\": [],\n",
        "    \"act_name\": \"event_activity\",\n",
        "    \"time_name\": \"event_timestamp\",\n",
        "    \"sep\": \",\",\n",
        "}\n",
        "file_path_object_attribute_table = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_val = EventSubGraphDataset(\n",
        "    root=STORAGE_PATH,\n",
        "    filename=FEATURE_STORAGE_FILE,\n",
        "    label_key=TARGET_LABEL,\n",
        "    size_subgraph_samples=4,\n",
        "    validation=True,\n",
        "    verbosity=51\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GCN(24, {\"num_hidden_features\": 24})\n",
        "loss_fn = torch.nn.L1Loss()\n",
        "model = model.to(device)\n",
        "val_loader = DataLoader(ds_val, batch_size=64, shuffle=True)\n",
        "running_vloss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tim/Development/OCELFeatureExtractionExperiments/.env/lib/python3.9/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n",
            "/home/tim/Development/OCELFeatureExtractionExperiments/.env/lib/python3.9/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([156])) that is different to the input size (torch.Size([156, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for i, vdata in enumerate(val_loader):\n",
        "            vdata.to(device)\n",
        "            vinputs, vadjacency_matrix, vlabels = (\n",
        "                vdata.x.float(),\n",
        "                vdata.edge_index,\n",
        "                vdata.y.float(),\n",
        "            )\n",
        "            voutputs = model(vinputs, vadjacency_matrix)\n",
        "            vloss = loss_fn(voutputs, vlabels)\n",
        "            running_vloss += vloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "15754it [01:22, 190.43it/s]\n",
            "Done!\n",
            "Processing...\n",
            "6302it [00:31, 200.10it/s]\n",
            "Done!\n",
            "Processing...\n",
            "9453it [00:49, 191.69it/s]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "ds_train = EventSubGraphDataset(\n",
        "    root=STORAGE_PATH,\n",
        "    filename=FEATURE_STORAGE_FILE,\n",
        "    label_key=TARGET_LABEL,\n",
        "    size_subgraph_samples=4,\n",
        "    train=True,\n",
        "    verbosity=51\n",
        ")\n",
        "ds_val = EventSubGraphDataset(\n",
        "    root=STORAGE_PATH,\n",
        "    filename=FEATURE_STORAGE_FILE,\n",
        "    label_key=TARGET_LABEL,\n",
        "    size_subgraph_samples=4,\n",
        "    validation=True,\n",
        "    verbosity=51\n",
        ")\n",
        "ds_test = EventSubGraphDataset(\n",
        "    root=STORAGE_PATH,\n",
        "    filename=FEATURE_STORAGE_FILE,\n",
        "    label_key=TARGET_LABEL,\n",
        "    size_subgraph_samples=4,\n",
        "    test=True,\n",
        "    verbosity=51\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "150012"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds_train.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing OCEL\n",
        "ocel = csv_import_factory.apply(\n",
        "    filename, csv_import_factory.TO_OCEL, parameters, file_path_object_attribute_table\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f\"debug/self._mapping-pass.pkl\", \"rb\") as file:\n",
        "    mapping_pass = pickle.load(file)\n",
        "with open(\"debug/attribute-pass.pkl\", \"rb\") as file:\n",
        "    attribute_pass = pickle.load(file)\n",
        "with open(\"debug/e_id-pass.pkl\", \"rb\") as file:\n",
        "    e_id_pass = pickle.load(file)\n",
        "with open(\"debug/self._mapping[attribute][e_id]-pass.pkl\", \"rb\") as file:\n",
        "    value_pass = pickle.load(file)\n",
        "\n",
        "with open(f\"debug/self._mapping-fail.pkl\", \"rb\") as file:\n",
        "    mapping_fail = pickle.load(file)\n",
        "with open(\"debug/attribute-fail.pkl\", \"rb\") as file:\n",
        "    attribute_fail = pickle.load(file)\n",
        "with open(\"debug/e_id-fail.pkl\", \"rb\") as file:\n",
        "    e_id_fail = pickle.load(file)\n",
        "\n",
        "with open(\"debug/ocel.log\", \"rb\") as file:\n",
        "    ocel_log = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "map = pd.DataFrame(ocel.log._mapping)\n",
        "map.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ocel.parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mapping_pass = pd.DataFrame(mapping_pass)\n",
        "print(attribute_pass)\n",
        "print(e_id_pass)\n",
        "print(value_pass)\n",
        "mapping_pass.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mapping_fail = pd.DataFrame(mapping_fail)\n",
        "print(attribute_fail)\n",
        "print(e_id_fail)\n",
        "mapping_fail.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "            self._mapping[attribute][e_id]\n",
        "            with open(\"self._mapping-pass.pkl\", \"wb\") as file:\n",
        "                pickle.dump(self._mapping, file)\n",
        "            with open(\"attribute-pass.pkl\", \"wb\") as file:\n",
        "                pickle.dump(attribute, file)\n",
        "            with open(\"e_id-pass.pkl\", \"wb\") as file:\n",
        "                pickle.dump(e_id, file)\n",
        "            with open(\"self._mapping[attribute][e_id]-pass.pkl\", \"wb\") as file:\n",
        "                pickle.dump(self._mapping[attribute][e_id], file)\n",
        "        except:\n",
        "            with open(\"self._mapping-fail.pkl\", \"wb\") as file:\n",
        "                pickle.dump(self._mapping, file)\n",
        "            with open(\"attribute-fail.pkl\", \"wb\") as file:\n",
        "                pickle.dump(attribute, file)\n",
        "            with open(\"e_id-fail.pkl\", \"wb\") as file:\n",
        "                pickle.dump(e_id, file)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
